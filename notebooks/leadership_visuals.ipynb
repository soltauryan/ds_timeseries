{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Accuracy Review\n",
    "\n",
    "**Purpose**: Compare new model candidates against our current predictions.  \n",
    "**Test period**: FY start (Nov 25) through current week.  \n",
    "**Audience**: Leadership — all charts are presentation-ready.\n",
    "\n",
    "---\n",
    "\n",
    "### How to read this notebook\n",
    "\n",
    "1. **Run the walkthrough notebook first** (`walkthrough_your_data.ipynb`) to generate model predictions  \n",
    "2. This notebook **loads those results** and builds the visuals  \n",
    "3. Every chart can be exported to HTML (interactive) or PNG (for slides)  \n",
    "4. Sections are ordered for a top-down executive presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup & Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Consistent styling for all charts\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "from ds_timeseries.evaluation.metrics import wape, mae, bias, evaluate_forecast\n",
    "from ds_timeseries.features import add_fiscal_features, rollup_to_fiscal_month\n",
    "from ds_timeseries.evaluation.plots import (\n",
    "    plot_forecast,\n",
    "    plot_forecast_grid,\n",
    "    plot_model_comparison,\n",
    "    COLORS,\n",
    "    MODEL_COLORS,\n",
    ")\n",
    "\n",
    "# --- Color palette ---\n",
    "# Consistent across all leadership charts\n",
    "BRAND = {\n",
    "    \"green\":  \"#2D9F5A\",   # good / improvement\n",
    "    \"yellow\": \"#F5A623\",   # acceptable / caution\n",
    "    \"red\":    \"#D94F4F\",   # poor / needs attention\n",
    "    \"blue\":   \"#2E86AB\",   # primary / our new model\n",
    "    \"gray\":   \"#8B95A2\",   # benchmark / reference\n",
    "    \"dark\":   \"#2D3436\",   # text\n",
    "    \"light\":  \"#F8F9FA\",   # background\n",
    "}\n",
    "\n",
    "# Chart export settings\n",
    "EXPORT_DIR = \"../data/raw/charts\"  # change if you want\n",
    "CHART_HEIGHT = 500\n",
    "CHART_WIDTH = 900\n",
    "\n",
    "import os\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === EDIT THIS SECTION ===\n# Load data from the walkthrough notebook.\n# The walkthrough saves model_scorecard.csv. You need to also save/load:\n#   - test actuals\n#   - fiscal calendar\n#   - model predictions dict\n#\n# Easiest approach: run both notebooks in the same Jupyter session,\n# or save/load with parquet as shown below.\n\n# --- Your actuals (full dataset, for train history + test) ---\n# raw = pd.read_parquet(\"../data/raw/your_sales_data.parquet\")\n# raw[\"ds\"] = pd.to_datetime(raw[\"ds\"])\n# ... (same column mapping as walkthrough) ...\n\n# --- Or if you saved train/test separately: ---\n# test = pd.read_parquet(\"../data/raw/test_actuals.parquet\")\n# train = pd.read_parquet(\"../data/raw/train_actuals.parquet\")\n\n# --- Your fiscal calendar ---\n# fiscal_cal = pd.read_parquet(\"../data/raw/your_fiscal_calendar.parquet\")\n\n# --- Cutoff date ---\nCUTOFF_DATE = pd.Timestamp(\"2025-11-25\")\n\n# --- For dollar impact estimation ---\nAVG_UNIT_VALUE = 500   # average dollar value per unit of y\n# If y is already in dollars, set this to 1\n\n# --- Model predictions (dict of name -> DataFrame with unique_id, ds, yhat) ---\n# Collect all model predictions into this dictionary.\n# Each value is a DataFrame with columns: unique_id, ds, yhat\nall_predictions = {\n    # \"Your Predictions\": existing_preds,\n    # \"LightGBM\":         lgb_preds,\n    # \"XGBoost\":          xgb_preds,\n    # \"CatBoost\":         cat_preds,\n    # \"DRFAM\":            drfam_preds,\n    # \"SimpleEnsemble\":   simple_ens_preds,\n    # etc.\n}\n\n# --- Validate that required variables are loaded ---\nfor var_name in [\"test\", \"all_predictions\"]:\n    if var_name not in dir():\n        raise NameError(\n            f\"'{var_name}' is not defined. Load your data above before continuing. \"\n            f\"See walkthrough_your_data.ipynb for how to generate these.\"\n        )\n\nprint(f\"Models loaded: {list(all_predictions.keys())}\")\nprint(f\"Test period: {test['ds'].min().date()} to {test['ds'].max().date()}\")\nprint(f\"Test weeks: {test['ds'].nunique()}\")\nprint(f\"Series: {test['unique_id'].nunique()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score all models against actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a unified scores table\n",
    "scores_list = []\n",
    "\n",
    "for model_name, preds in all_predictions.items():\n",
    "    merged = test.merge(preds[[\"unique_id\", \"ds\", \"yhat\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "    if len(merged) == 0:\n",
    "        print(f\"  WARNING: {model_name} — no matching rows\")\n",
    "        continue\n",
    "    scores = evaluate_forecast(merged[\"y\"], merged[\"yhat\"])\n",
    "    scores[\"model\"] = model_name\n",
    "    scores[\"n_rows\"] = len(merged)\n",
    "    scores_list.append(scores)\n",
    "\n",
    "scorecard = pd.DataFrame(scores_list).sort_values(\"wape\").reset_index(drop=True)\n",
    "scorecard[\"rank\"] = range(1, len(scorecard) + 1)\n",
    "\n",
    "# Flag \"Your Predictions\" as the benchmark\n",
    "benchmark_wape = scorecard.loc[scorecard[\"model\"] == \"Your Predictions\", \"wape\"].values\n",
    "if len(benchmark_wape) > 0:\n",
    "    benchmark_wape = benchmark_wape[0]\n",
    "    scorecard[\"vs_benchmark_pct\"] = -(scorecard[\"wape\"] - benchmark_wape) / benchmark_wape * 100\n",
    "else:\n",
    "    benchmark_wape = scorecard[\"wape\"].max()\n",
    "    scorecard[\"vs_benchmark_pct\"] = np.nan\n",
    "\n",
    "scorecard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Executive Summary — The Headline Number\n",
    "\n",
    "One slide. One number. The improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = scorecard.loc[scorecard[\"model\"] != \"Your Predictions\"].iloc[0]\n",
    "improvement_pct = (benchmark_wape - best_model[\"wape\"]) / benchmark_wape * 100\n",
    "improvement_abs = benchmark_wape - best_model[\"wape\"]\n",
    "\n",
    "# Estimate dollar impact\n",
    "annual_volume = test[\"y\"].sum()  # total units in test period\n",
    "weeks_in_test = test[\"ds\"].nunique()\n",
    "annual_volume_projected = annual_volume * (52 / max(weeks_in_test, 1))\n",
    "dollar_impact = annual_volume_projected * AVG_UNIT_VALUE * improvement_abs\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Big number card\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"number+delta\",\n",
    "    value=best_model[\"wape\"] * 100,\n",
    "    number={\"suffix\": \"%\", \"font\": {\"size\": 72, \"color\": BRAND[\"blue\"]}},\n",
    "    delta={\n",
    "        \"reference\": benchmark_wape * 100,\n",
    "        \"decreasing\": {\"color\": BRAND[\"green\"]},\n",
    "        \"increasing\": {\"color\": BRAND[\"red\"]},\n",
    "        \"suffix\": \" pts\",\n",
    "        \"font\": {\"size\": 28},\n",
    "    },\n",
    "    title={\n",
    "        \"text\": (\n",
    "            f\"<b>Forecast Accuracy (WAPE)</b><br>\"\n",
    "            f\"<span style='font-size:16px;color:{BRAND['gray']}'>Best Model: {best_model['model']}</span>\"\n",
    "        ),\n",
    "        \"font\": {\"size\": 20, \"color\": BRAND[\"dark\"]},\n",
    "    },\n",
    "    domain={\"x\": [0, 0.5], \"y\": [0, 1]},\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"number\",\n",
    "    value=dollar_impact,\n",
    "    number={\"prefix\": \"$\", \"font\": {\"size\": 56, \"color\": BRAND[\"green\"]}, \"valueformat\": \",.0f\"},\n",
    "    title={\n",
    "        \"text\": (\n",
    "            f\"<b>Estimated Annual Impact</b><br>\"\n",
    "            f\"<span style='font-size:14px;color:{BRAND['gray']}'>Projected from {weeks_in_test}-week test period</span>\"\n",
    "        ),\n",
    "        \"font\": {\"size\": 20, \"color\": BRAND[\"dark\"]},\n",
    "    },\n",
    "    domain={\"x\": [0.55, 1], \"y\": [0, 1]},\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    height=300,\n",
    "    margin=dict(t=80, b=20, l=20, r=20),\n",
    "    paper_bgcolor=BRAND[\"light\"],\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/01_executive_summary.html\")\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nTalking point: 'We reduced forecast error from {benchmark_wape:.1%} to {best_model['wape']:.1%}\")\n",
    "print(f\"               — a {improvement_pct:.0f}% improvement, worth an estimated ${dollar_impact:,.0f}/year.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Comparison — Which Approach Wins?\n",
    "\n",
    "Horizontal bar chart. Your current predictions as a red dashed line.  \n",
    "Everything to the left of the line is an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = scorecard.sort_values(\"wape\", ascending=True).copy()\n",
    "\n",
    "# Color: green if better than benchmark, gray if benchmark, red if worse\n",
    "def bar_color(row):\n",
    "    if row[\"model\"] == \"Your Predictions\":\n",
    "        return BRAND[\"gray\"]\n",
    "    return BRAND[\"green\"] if row[\"wape\"] < benchmark_wape else BRAND[\"red\"]\n",
    "\n",
    "sc[\"color\"] = sc.apply(bar_color, axis=1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=sc[\"model\"],\n",
    "    x=sc[\"wape\"],\n",
    "    orientation=\"h\",\n",
    "    marker_color=sc[\"color\"],\n",
    "    text=[f\"{w:.1%}\" for w in sc[\"wape\"]],\n",
    "    textposition=\"outside\",\n",
    "    textfont=dict(size=13),\n",
    "    hovertemplate=\"%{y}<br>WAPE: %{x:.2%}<extra></extra>\",\n",
    "))\n",
    "\n",
    "# Benchmark reference line\n",
    "fig.add_vline(\n",
    "    x=benchmark_wape,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=BRAND[\"red\"],\n",
    "    line_width=2,\n",
    "    annotation_text=f\"Current: {benchmark_wape:.1%}\",\n",
    "    annotation_position=\"top\",\n",
    "    annotation_font=dict(size=13, color=BRAND[\"red\"]),\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Forecast Accuracy by Model</b><br>\"\n",
    "             \"<span style='font-size:14px;color:gray'>WAPE — lower is better. Green = beats current predictions.</span>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Weighted Absolute % Error (WAPE)\",\n",
    "        tickformat=\".0%\",\n",
    "        gridcolor=\"#EEEEEE\",\n",
    "        range=[0, max(sc[\"wape\"]) * 1.2],\n",
    "    ),\n",
    "    yaxis=dict(title=\"\"),\n",
    "    height=max(350, len(sc) * 40 + 120),\n",
    "    width=CHART_WIDTH,\n",
    "    margin=dict(l=180, r=80, t=100, b=60),\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/02_model_comparison.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Accuracy Trendline — Week-by-Week Performance\n",
    "\n",
    "Shows how each model's accuracy tracks over time against actuals.  \n",
    "Leadership can see: \"Is the model consistently good, or did it get lucky in one week?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weekly WAPE for each model\n",
    "weekly_scores = []\n",
    "\n",
    "for model_name, preds in all_predictions.items():\n",
    "    merged = test.merge(preds[[\"unique_id\", \"ds\", \"yhat\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "    for week_date, week_df in merged.groupby(\"ds\"):\n",
    "        total_actual = week_df[\"y\"].abs().sum()\n",
    "        if total_actual > 0:\n",
    "            w = (week_df[\"y\"] - week_df[\"yhat\"]).abs().sum() / total_actual\n",
    "        else:\n",
    "            w = np.nan\n",
    "        weekly_scores.append({\n",
    "            \"model\": model_name,\n",
    "            \"ds\": week_date,\n",
    "            \"wape\": w,\n",
    "            \"total_actual\": total_actual,\n",
    "            \"total_error\": (week_df[\"y\"] - week_df[\"yhat\"]).abs().sum(),\n",
    "        })\n",
    "\n",
    "weekly_df = pd.DataFrame(weekly_scores)\n",
    "weekly_df[\"ds\"] = pd.to_datetime(weekly_df[\"ds\"])\n",
    "\n",
    "print(f\"Weekly scores: {len(weekly_df)} rows\")\n",
    "print(f\"Models: {weekly_df['model'].nunique()}\")\n",
    "print(f\"Weeks: {weekly_df['ds'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Weekly WAPE trendline: all models ---\n",
    "fig = go.Figure()\n",
    "\n",
    "# Sort models by overall WAPE so legend is ordered best-to-worst\n",
    "model_order = scorecard.sort_values(\"wape\")[\"model\"].tolist()\n",
    "\n",
    "for idx, model_name in enumerate(model_order):\n",
    "    model_weekly = weekly_df[weekly_df[\"model\"] == model_name].sort_values(\"ds\")\n",
    "\n",
    "    is_benchmark = model_name == \"Your Predictions\"\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=model_weekly[\"ds\"],\n",
    "        y=model_weekly[\"wape\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=model_name,\n",
    "        line=dict(\n",
    "            color=BRAND[\"gray\"] if is_benchmark else MODEL_COLORS[idx % len(MODEL_COLORS)],\n",
    "            width=3 if is_benchmark else 2,\n",
    "            dash=\"dash\" if is_benchmark else \"solid\",\n",
    "        ),\n",
    "        marker=dict(size=6 if is_benchmark else 5),\n",
    "        hovertemplate=f\"{model_name}<br>Week: %{{x|%b %d}}<br>WAPE: %{{y:.1%}}<extra></extra>\",\n",
    "        opacity=1.0 if is_benchmark or idx < 3 else 0.5,\n",
    "    ))\n",
    "\n",
    "# Add fiscal month boundaries from your calendar\n",
    "try:\n",
    "    month_ends = fiscal_cal[fiscal_cal[\"is_fiscal_month_end\"]][\"ds\"]\n",
    "    month_ends_in_range = month_ends[\n",
    "        (month_ends >= test[\"ds\"].min()) & (month_ends <= test[\"ds\"].max())\n",
    "    ]\n",
    "    for me in month_ends_in_range:\n",
    "        fig.add_vline(\n",
    "            x=me, line_dash=\"dot\", line_color=\"#DDDDDD\", line_width=1,\n",
    "        )\n",
    "except NameError:\n",
    "    pass  # fiscal_cal not loaded\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Weekly Forecast Accuracy Over Time</b><br>\"\n",
    "             \"<span style='font-size:14px;color:gray'>WAPE per week — lower is better. \"\n",
    "             \"Dashed gray = current predictions.</span>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Week\",\n",
    "        tickformat=\"%b %d\",\n",
    "        gridcolor=\"#EEEEEE\",\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"WAPE (lower = more accurate)\",\n",
    "        tickformat=\".0%\",\n",
    "        gridcolor=\"#EEEEEE\",\n",
    "    ),\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0),\n",
    "    hovermode=\"x unified\",\n",
    "    height=CHART_HEIGHT,\n",
    "    width=CHART_WIDTH,\n",
    "    margin=dict(t=120, b=60),\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/03_weekly_accuracy_trendline.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cumulative (rolling) WAPE: shows convergence over time ---\n",
    "fig = go.Figure()\n",
    "\n",
    "for idx, model_name in enumerate(model_order):\n",
    "    model_weekly = weekly_df[weekly_df[\"model\"] == model_name].sort_values(\"ds\")\n",
    "    # Cumulative WAPE = cumulative error / cumulative actual\n",
    "    model_weekly = model_weekly.copy()\n",
    "    model_weekly[\"cum_error\"] = model_weekly[\"total_error\"].cumsum()\n",
    "    model_weekly[\"cum_actual\"] = model_weekly[\"total_actual\"].cumsum()\n",
    "    model_weekly[\"cum_wape\"] = model_weekly[\"cum_error\"] / model_weekly[\"cum_actual\"]\n",
    "\n",
    "    is_benchmark = model_name == \"Your Predictions\"\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=model_weekly[\"ds\"],\n",
    "        y=model_weekly[\"cum_wape\"],\n",
    "        mode=\"lines\",\n",
    "        name=model_name,\n",
    "        line=dict(\n",
    "            color=BRAND[\"gray\"] if is_benchmark else MODEL_COLORS[idx % len(MODEL_COLORS)],\n",
    "            width=3 if is_benchmark else 2,\n",
    "            dash=\"dash\" if is_benchmark else \"solid\",\n",
    "        ),\n",
    "        hovertemplate=f\"{model_name}<br>Through: %{{x|%b %d}}<br>Cumulative WAPE: %{{y:.1%}}<extra></extra>\",\n",
    "        opacity=1.0 if is_benchmark or idx < 3 else 0.5,\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Cumulative Accuracy Over Time</b><br>\"\n",
    "             \"<span style='font-size:14px;color:gray'>Running WAPE — stabilizes as more weeks are included. \"\n",
    "             \"Dashed gray = current predictions.</span>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    xaxis=dict(title=\"Through Week\", tickformat=\"%b %d\", gridcolor=\"#EEEEEE\"),\n",
    "    yaxis=dict(title=\"Cumulative WAPE\", tickformat=\".0%\", gridcolor=\"#EEEEEE\"),\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0),\n",
    "    hovermode=\"x unified\",\n",
    "    height=CHART_HEIGHT,\n",
    "    width=CHART_WIDTH,\n",
    "    margin=dict(t=120, b=60),\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/04_cumulative_accuracy.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Fiscal Month Accuracy — The Monthly View\n",
    "\n",
    "Leadership thinks in fiscal months. This rolls up weekly accuracy to show  \n",
    "how each model performed per fiscal period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll up each model's predictions to fiscal months\n",
    "monthly_scores = []\n",
    "\n",
    "for model_name, preds in all_predictions.items():\n",
    "    merged = test.merge(preds[[\"unique_id\", \"ds\", \"yhat\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "    try:\n",
    "        monthly = rollup_to_fiscal_month(\n",
    "            merged, value_cols=[\"y\", \"yhat\"], fiscal_calendar=fiscal_cal\n",
    "        )\n",
    "        # Aggregate across all series per fiscal month\n",
    "        month_agg = (\n",
    "            monthly\n",
    "            .groupby([\"fiscal_year\", \"fiscal_month\"])\n",
    "            .agg({\"y\": \"sum\", \"yhat\": \"sum\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        month_agg[\"wape\"] = (month_agg[\"y\"] - month_agg[\"yhat\"]).abs() / month_agg[\"y\"].abs()\n",
    "        month_agg[\"model\"] = model_name\n",
    "        month_agg[\"period_label\"] = \"FY\" + month_agg[\"fiscal_year\"].astype(str) + \" M\" + month_agg[\"fiscal_month\"].astype(str).str.zfill(2)\n",
    "        monthly_scores.append(month_agg)\n",
    "    except Exception as e:\n",
    "        print(f\"  {model_name}: Could not roll up — {e}\")\n",
    "\n",
    "monthly_all = pd.concat(monthly_scores, ignore_index=True)\n",
    "print(f\"Fiscal months with data: {monthly_all['period_label'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grouped bar chart: WAPE by fiscal month, grouped by model ---\n",
    "# Only show top N models to keep it readable\n",
    "TOP_N_MODELS = 5\n",
    "top_models = scorecard.head(TOP_N_MODELS)[\"model\"].tolist()\n",
    "if \"Your Predictions\" not in top_models:\n",
    "    top_models.append(\"Your Predictions\")\n",
    "\n",
    "monthly_top = monthly_all[monthly_all[\"model\"].isin(top_models)].copy()\n",
    "periods = sorted(monthly_top[\"period_label\"].unique())\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for idx, model_name in enumerate(top_models):\n",
    "    model_data = monthly_top[monthly_top[\"model\"] == model_name].set_index(\"period_label\")\n",
    "\n",
    "    is_benchmark = model_name == \"Your Predictions\"\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=model_name,\n",
    "        x=periods,\n",
    "        y=[model_data.loc[p, \"wape\"] if p in model_data.index else np.nan for p in periods],\n",
    "        marker_color=BRAND[\"gray\"] if is_benchmark else MODEL_COLORS[idx % len(MODEL_COLORS)],\n",
    "        marker_pattern_shape=\"/\" if is_benchmark else \"\",\n",
    "        hovertemplate=f\"{model_name}<br>%{{x}}<br>WAPE: %{{y:.1%}}<extra></extra>\",\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"group\",\n",
    "    title=dict(\n",
    "        text=\"<b>Accuracy by Fiscal Month</b><br>\"\n",
    "             \"<span style='font-size:14px;color:gray'>WAPE per fiscal month — lower is better. \"\n",
    "             \"Hatched bars = current predictions.</span>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    xaxis=dict(title=\"Fiscal Period\", gridcolor=\"#EEEEEE\"),\n",
    "    yaxis=dict(title=\"WAPE\", tickformat=\".0%\", gridcolor=\"#EEEEEE\"),\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0),\n",
    "    height=CHART_HEIGHT,\n",
    "    width=CHART_WIDTH,\n",
    "    margin=dict(t=120, b=60),\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/05_fiscal_month_accuracy.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Traffic Light Summary — At a Glance\n",
    "\n",
    "How many Customer-Material combinations fall into each accuracy bucket?  \n",
    "Simple red/yellow/green that operations teams can act on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare traffic light distributions: Your Predictions vs Best Model\n",
    "compare_models = [\"Your Predictions\", best_model[\"model\"]]\n",
    "\n",
    "traffic_data = []\n",
    "\n",
    "for model_name in compare_models:\n",
    "    if model_name not in all_predictions:\n",
    "        continue\n",
    "    preds = all_predictions[model_name]\n",
    "    merged = test.merge(preds[[\"unique_id\", \"ds\", \"yhat\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "\n",
    "    # Per-series WAPE\n",
    "    series_wape = (\n",
    "        merged.groupby(\"unique_id\")\n",
    "        .apply(lambda x: (x[\"y\"] - x[\"yhat\"]).abs().sum() / max(x[\"y\"].abs().sum(), 1e-6), include_groups=False)\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"wape\"})\n",
    "    )\n",
    "\n",
    "    # Traffic light buckets\n",
    "    series_wape[\"bucket\"] = pd.cut(\n",
    "        series_wape[\"wape\"],\n",
    "        bins=[0, 0.35, 0.50, float(\"inf\")],\n",
    "        labels=[\"Good (<35%)\", \"Acceptable (35-50%)\", \"Needs Attention (>50%)\"],\n",
    "    )\n",
    "    counts = series_wape[\"bucket\"].value_counts().to_dict()\n",
    "    total = len(series_wape)\n",
    "\n",
    "    for bucket, count in counts.items():\n",
    "        traffic_data.append({\n",
    "            \"model\": model_name,\n",
    "            \"bucket\": bucket,\n",
    "            \"count\": count,\n",
    "            \"pct\": count / total,\n",
    "        })\n",
    "\n",
    "traffic_df = pd.DataFrame(traffic_data)\n",
    "\n",
    "# Stacked bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "bucket_colors = {\n",
    "    \"Good (<35%)\": BRAND[\"green\"],\n",
    "    \"Acceptable (35-50%)\": BRAND[\"yellow\"],\n",
    "    \"Needs Attention (>50%)\": BRAND[\"red\"],\n",
    "}\n",
    "\n",
    "for bucket_name in [\"Good (<35%)\", \"Acceptable (35-50%)\", \"Needs Attention (>50%)\"]:\n",
    "    bucket_data = traffic_df[traffic_df[\"bucket\"] == bucket_name]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=bucket_name,\n",
    "        y=bucket_data[\"model\"],\n",
    "        x=bucket_data[\"pct\"],\n",
    "        orientation=\"h\",\n",
    "        marker_color=bucket_colors[bucket_name],\n",
    "        text=[f\"{p:.0%} ({c})\" for p, c in zip(bucket_data[\"pct\"], bucket_data[\"count\"])],\n",
    "        textposition=\"inside\",\n",
    "        textfont=dict(size=14, color=\"white\"),\n",
    "        hovertemplate=f\"{bucket_name}<br>%{{y}}<br>%{{x:.1%}} ({bucket_data['count'].values[0] if len(bucket_data) > 0 else 0} series)<extra></extra>\",\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    title=dict(\n",
    "        text=\"<b>Accuracy Distribution: Current vs Best Model</b><br>\"\n",
    "             \"<span style='font-size:14px;color:gray'>% of Customer-Material combinations in each accuracy bucket</span>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    xaxis=dict(title=\"% of Series\", tickformat=\".0%\", gridcolor=\"#EEEEEE\", range=[0, 1]),\n",
    "    yaxis=dict(title=\"\"),\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"center\", x=0.5),\n",
    "    height=300,\n",
    "    width=CHART_WIDTH,\n",
    "    margin=dict(l=200, r=40, t=120, b=60),\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/06_traffic_light.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Forecast vs Actual — Top Products\n",
    "\n",
    "Pick the highest-volume Customer-Material pairs that leadership knows.  \n",
    "Show the forecast tracking actuals week by week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 6 series by total volume in test period\n",
    "top_series = (\n",
    "    test.groupby(\"unique_id\")[\"y\"].sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(6)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "print(f\"Top {len(top_series)} series by volume:\")\n",
    "for s in top_series:\n",
    "    vol = test[test[\"unique_id\"] == s][\"y\"].sum()\n",
    "    print(f\"  {s}: {vol:,.0f} total units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a comparison dict: Your Predictions vs Best Model\n",
    "comparison_preds = {}\n",
    "if \"Your Predictions\" in all_predictions:\n",
    "    comparison_preds[\"Current\"] = all_predictions[\"Your Predictions\"]\n",
    "comparison_preds[best_model[\"model\"]] = all_predictions[best_model[\"model\"]]\n",
    "\n",
    "# Also need full actuals (train + test) for historical context\n",
    "# If you have the full df loaded:\n",
    "try:\n",
    "    full_actuals = pd.concat([train[[\"unique_id\", \"ds\", \"y\"]], test[[\"unique_id\", \"ds\", \"y\"]]])\n",
    "except NameError:\n",
    "    full_actuals = test[[\"unique_id\", \"ds\", \"y\"]]  # fallback to test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x3 grid: top 6 products, showing Current vs Best Model vs Actuals\n",
    "n_cols = 3\n",
    "n_rows = 2\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=n_rows, cols=n_cols,\n",
    "    subplot_titles=[s[:40] for s in top_series],\n",
    "    vertical_spacing=0.12,\n",
    "    horizontal_spacing=0.06,\n",
    ")\n",
    "\n",
    "for idx, series_id in enumerate(top_series):\n",
    "    row = idx // n_cols + 1\n",
    "    col = idx % n_cols + 1\n",
    "\n",
    "    # Historical (last 26 weeks before cutoff)\n",
    "    series_hist = full_actuals[\n",
    "        (full_actuals[\"unique_id\"] == series_id) & (full_actuals[\"ds\"] < CUTOFF_DATE)\n",
    "    ].sort_values(\"ds\").tail(26)\n",
    "\n",
    "    # Actuals in test period\n",
    "    series_actual = test[test[\"unique_id\"] == series_id].sort_values(\"ds\")\n",
    "\n",
    "    # Historical line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=series_hist[\"ds\"], y=series_hist[\"y\"],\n",
    "        mode=\"lines\", line=dict(color=BRAND[\"dark\"], width=2),\n",
    "        name=\"Historical\", showlegend=(idx == 0),\n",
    "        legendgroup=\"hist\",\n",
    "        hovertemplate=\"%{y:.0f}<extra>Historical</extra>\",\n",
    "    ), row=row, col=col)\n",
    "\n",
    "    # Actual in test period\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=series_actual[\"ds\"], y=series_actual[\"y\"],\n",
    "        mode=\"lines+markers\", line=dict(color=BRAND[\"dark\"], width=2, dash=\"dot\"),\n",
    "        marker=dict(size=5),\n",
    "        name=\"Actual\", showlegend=(idx == 0),\n",
    "        legendgroup=\"actual\",\n",
    "        hovertemplate=\"%{y:.0f}<extra>Actual</extra>\",\n",
    "    ), row=row, col=col)\n",
    "\n",
    "    # Each model's forecast\n",
    "    for m_idx, (m_name, m_preds) in enumerate(comparison_preds.items()):\n",
    "        series_pred = m_preds[m_preds[\"unique_id\"] == series_id].sort_values(\"ds\")\n",
    "        color = BRAND[\"gray\"] if m_name == \"Current\" else BRAND[\"blue\"]\n",
    "        dash = \"dash\" if m_name == \"Current\" else \"solid\"\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=series_pred[\"ds\"], y=series_pred[\"yhat\"],\n",
    "            mode=\"lines\", line=dict(color=color, width=2, dash=dash),\n",
    "            name=m_name, showlegend=(idx == 0),\n",
    "            legendgroup=m_name,\n",
    "            hovertemplate=f\"%{{y:.0f}}<extra>{m_name}</extra>\",\n",
    "        ), row=row, col=col)\n",
    "\n",
    "    # Cutoff line\n",
    "    fig.add_vline(x=CUTOFF_DATE, line_dash=\"dot\", line_color=\"#CCCCCC\", row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Top Products: Forecast vs Actual</b><br>\"\n",
    "             \"<span style='font-size:14px;color:gray'>Solid dark = actual, \"\n",
    "             \"dashed gray = current predictions, solid blue = new model</span>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.06, xanchor=\"left\", x=0),\n",
    "    height=600,\n",
    "    width=CHART_WIDTH + 100,\n",
    "    margin=dict(t=140, b=40),\n",
    ")\n",
    "fig.update_xaxes(tickformat=\"%b %d\", gridcolor=\"#EEEEEE\")\n",
    "fig.update_yaxes(gridcolor=\"#EEEEEE\")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/07_top_products_forecast_vs_actual.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Biggest Improvements & Biggest Misses\n",
    "\n",
    "Where did the new model help the most?  \n",
    "Where does it still struggle? (Actionable for ops.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Your Predictions\" in all_predictions:\n",
    "    benchmark_preds = all_predictions[\"Your Predictions\"]\n",
    "    best_preds = all_predictions[best_model[\"model\"]]\n",
    "\n",
    "    # Per-series WAPE for both\n",
    "    def series_wapes(preds):\n",
    "        merged = test.merge(preds[[\"unique_id\", \"ds\", \"yhat\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "        return (\n",
    "            merged.groupby(\"unique_id\")\n",
    "            .apply(lambda x: (x[\"y\"] - x[\"yhat\"]).abs().sum() / max(x[\"y\"].abs().sum(), 1e-6), include_groups=False)\n",
    "            .rename(\"wape\")\n",
    "        )\n",
    "\n",
    "    wape_current = series_wapes(benchmark_preds)\n",
    "    wape_new = series_wapes(best_preds)\n",
    "\n",
    "    comparison = pd.DataFrame({\n",
    "        \"current_wape\": wape_current,\n",
    "        \"new_wape\": wape_new,\n",
    "    }).dropna()\n",
    "    comparison[\"improvement\"] = comparison[\"current_wape\"] - comparison[\"new_wape\"]\n",
    "    comparison[\"volume\"] = test.groupby(\"unique_id\")[\"y\"].sum()\n",
    "\n",
    "    # Top 15 improvements\n",
    "    top_improvements = comparison.sort_values(\"improvement\", ascending=False).head(15)\n",
    "    # Top 15 regressions (where new model is worse)\n",
    "    top_regressions = comparison.sort_values(\"improvement\", ascending=True).head(15)\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Biggest Improvements\", \"Needs Investigation\"],\n",
    "        horizontal_spacing=0.15,\n",
    "    )\n",
    "\n",
    "    # Improvements\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=[s[:25] for s in top_improvements.index],\n",
    "        x=top_improvements[\"improvement\"],\n",
    "        orientation=\"h\",\n",
    "        marker_color=BRAND[\"green\"],\n",
    "        text=[f\"+{v:.0%}\" for v in top_improvements[\"improvement\"]],\n",
    "        textposition=\"outside\",\n",
    "        hovertemplate=\"%{y}<br>Improvement: %{x:.1%}<extra></extra>\",\n",
    "        showlegend=False,\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Regressions\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=[s[:25] for s in top_regressions.index],\n",
    "        x=top_regressions[\"improvement\"].abs(),\n",
    "        orientation=\"h\",\n",
    "        marker_color=BRAND[\"red\"],\n",
    "        text=[f\"-{v:.0%}\" for v in top_regressions[\"improvement\"].abs()],\n",
    "        textposition=\"outside\",\n",
    "        hovertemplate=\"%{y}<br>Regression: %{x:.1%}<extra></extra>\",\n",
    "        showlegend=False,\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"<b>Where {best_model['model']} Helps vs Hurts</b><br>\"\n",
    "                 \"<span style='font-size:14px;color:gray'>WAPE improvement per Customer-Material pair</span>\",\n",
    "            font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "        ),\n",
    "        height=550,\n",
    "        width=CHART_WIDTH + 100,\n",
    "        margin=dict(l=200, r=80, t=100, b=40),\n",
    "    )\n",
    "    fig.update_xaxes(tickformat=\".0%\", gridcolor=\"#EEEEEE\")\n",
    "    fig.update_yaxes(gridcolor=\"#EEEEEE\")\n",
    "\n",
    "    fig.write_html(f\"{EXPORT_DIR}/08_improvements_and_misses.html\")\n",
    "    fig.show()\n",
    "\n",
    "    print(f\"\\n{(comparison['improvement'] > 0).sum()} of {len(comparison)} series improved\")\n",
    "    print(f\"{(comparison['improvement'] < 0).sum()} series regressed\")\n",
    "    print(f\"{(comparison['improvement'] == 0).sum()} series unchanged\")\n",
    "else:\n",
    "    print(\"No benchmark predictions loaded — skipping improvement analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Accuracy by Hierarchy Level\n",
    "\n",
    "Roll up accuracy to each level of the hierarchy.  \n",
    "Shows whether the model is better at the aggregate or granular level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EDIT THIS: hierarchy columns in your test data ===\n",
    "# These should match your data. Comment out any you don't have.\n",
    "hierarchy_levels = {\n",
    "    \"Total\":           None,  # all data\n",
    "    \"Parent Customer\": \"parent_customer_id\",\n",
    "    \"Customer\":        \"customer_id\",\n",
    "    \"Profit Center\":   \"profit_center_id\",\n",
    "    \"Material\":        \"material_id\",\n",
    "    \"Cust-Material\":   \"unique_id\",\n",
    "}\n",
    "\n",
    "hier_scores = []\n",
    "\n",
    "for model_name in compare_models:\n",
    "    if model_name not in all_predictions:\n",
    "        continue\n",
    "    preds = all_predictions[model_name]\n",
    "    merged = test.merge(preds[[\"unique_id\", \"ds\", \"yhat\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "\n",
    "    for level_name, level_col in hierarchy_levels.items():\n",
    "        if level_col is not None and level_col not in merged.columns:\n",
    "            continue\n",
    "        if level_col is None:\n",
    "            # Total level\n",
    "            w = wape(merged[\"y\"], merged[\"yhat\"])\n",
    "            n_groups = 1\n",
    "        else:\n",
    "            # Per-group, then average (weighted by volume)\n",
    "            group_wapes = (\n",
    "                merged.groupby(level_col)\n",
    "                .apply(lambda x: (x[\"y\"] - x[\"yhat\"]).abs().sum() / max(x[\"y\"].abs().sum(), 1e-6), include_groups=False)\n",
    "            )\n",
    "            w = wape(merged[\"y\"], merged[\"yhat\"])  # volume-weighted\n",
    "            n_groups = len(group_wapes)\n",
    "\n",
    "        hier_scores.append({\n",
    "            \"model\": model_name,\n",
    "            \"level\": level_name,\n",
    "            \"wape\": w,\n",
    "            \"n_groups\": n_groups,\n",
    "        })\n",
    "\n",
    "hier_df = pd.DataFrame(hier_scores)\n",
    "\n",
    "if len(hier_df) > 0:\n",
    "    fig = px.bar(\n",
    "        hier_df,\n",
    "        x=\"level\", y=\"wape\", color=\"model\",\n",
    "        barmode=\"group\",\n",
    "        text_auto=\".1%\",\n",
    "        color_discrete_map={\n",
    "            compare_models[0]: BRAND[\"gray\"] if compare_models[0] == \"Your Predictions\" else BRAND[\"blue\"],\n",
    "            compare_models[-1]: BRAND[\"blue\"],\n",
    "        } if len(compare_models) == 2 else None,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"<b>Accuracy by Hierarchy Level</b><br>\"\n",
    "                 \"<span style='font-size:14px;color:gray'>WAPE at each aggregation level — \"\n",
    "                 \"accuracy improves as you aggregate up</span>\",\n",
    "            font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "        ),\n",
    "        xaxis=dict(title=\"\"),\n",
    "        yaxis=dict(title=\"WAPE\", tickformat=\".0%\", gridcolor=\"#EEEEEE\"),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0,\n",
    "                    title=\"\"),\n",
    "        height=CHART_HEIGHT,\n",
    "        width=CHART_WIDTH,\n",
    "        margin=dict(t=120, b=60),\n",
    "    )\n",
    "    fig.write_html(f\"{EXPORT_DIR}/09_accuracy_by_hierarchy.html\")\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No hierarchy columns found in test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Bias Analysis — Over-Forecasting vs Under-Forecasting\n",
    "\n",
    "Leadership cares about direction. Over-forecasting means excess inventory.  \n",
    "Under-forecasting means stockouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly bias (forecast - actual): positive = over-forecast\n",
    "bias_data = []\n",
    "\n",
    "for model_name in compare_models:\n",
    "    if model_name not in all_predictions:\n",
    "        continue\n",
    "    preds = all_predictions[model_name]\n",
    "    merged = test.merge(preds[[\"unique_id\", \"ds\", \"yhat\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "\n",
    "    for week_date, week_df in merged.groupby(\"ds\"):\n",
    "        total_bias = (week_df[\"yhat\"] - week_df[\"y\"]).sum()\n",
    "        total_actual = week_df[\"y\"].sum()\n",
    "        bias_data.append({\n",
    "            \"model\": model_name,\n",
    "            \"ds\": week_date,\n",
    "            \"bias_units\": total_bias,\n",
    "            \"bias_pct\": total_bias / max(abs(total_actual), 1e-6),\n",
    "            \"total_actual\": total_actual,\n",
    "        })\n",
    "\n",
    "bias_df = pd.DataFrame(bias_data)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=[\"Weekly Bias (% of Actual)\", \"Cumulative Bias Direction\"],\n",
    "    vertical_spacing=0.15,\n",
    ")\n",
    "\n",
    "for idx, model_name in enumerate(compare_models):\n",
    "    if model_name not in all_predictions:\n",
    "        continue\n",
    "    model_bias = bias_df[bias_df[\"model\"] == model_name].sort_values(\"ds\").copy()\n",
    "\n",
    "    is_benchmark = model_name == \"Your Predictions\"\n",
    "    color = BRAND[\"gray\"] if is_benchmark else BRAND[\"blue\"]\n",
    "\n",
    "    # Weekly bias\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=model_bias[\"ds\"],\n",
    "        y=model_bias[\"bias_pct\"],\n",
    "        name=model_name,\n",
    "        marker_color=[BRAND[\"red\"] if b > 0 else BRAND[\"green\"] for b in model_bias[\"bias_pct\"]],\n",
    "        opacity=0.4 if is_benchmark else 0.7,\n",
    "        showlegend=False,\n",
    "        hovertemplate=f\"{model_name}<br>%{{x|%b %d}}<br>Bias: %{{y:.1%}}<extra></extra>\",\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Cumulative bias\n",
    "    model_bias[\"cum_bias\"] = model_bias[\"bias_units\"].cumsum()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=model_bias[\"ds\"],\n",
    "        y=model_bias[\"cum_bias\"],\n",
    "        mode=\"lines\",\n",
    "        name=model_name,\n",
    "        line=dict(color=color, width=2.5, dash=\"dash\" if is_benchmark else \"solid\"),\n",
    "        hovertemplate=f\"{model_name}<br>Through: %{{x|%b %d}}<br>Cum. Bias: %{{y:,.0f}} units<extra></extra>\",\n",
    "    ), row=2, col=1)\n",
    "\n",
    "fig.add_hline(y=0, line_dash=\"solid\", line_color=\"#CCCCCC\", row=1, col=1)\n",
    "fig.add_hline(y=0, line_dash=\"solid\", line_color=\"#CCCCCC\", row=2, col=1)\n",
    "\n",
    "# Annotations\n",
    "fig.add_annotation(\n",
    "    text=\"Over-forecasting (excess inventory)\",\n",
    "    xref=\"paper\", yref=\"y\", x=1.02, y=0.05,\n",
    "    showarrow=False, font=dict(size=11, color=BRAND[\"red\"]),\n",
    "    textangle=-90, row=1, col=1,\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=\"Under-forecasting (stockout risk)\",\n",
    "    xref=\"paper\", yref=\"y\", x=1.02, y=-0.05,\n",
    "    showarrow=False, font=dict(size=11, color=BRAND[\"green\"]),\n",
    "    textangle=-90, row=1, col=1,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Forecast Bias Analysis</b><br>\"\n",
    "             \"<span style='font-size:14px;color:gray'>Red = over-forecast (excess inventory), \"\n",
    "             \"green = under-forecast (stockout risk)</span>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    height=650,\n",
    "    width=CHART_WIDTH,\n",
    "    margin=dict(t=100, b=40, r=80),\n",
    ")\n",
    "fig.update_xaxes(tickformat=\"%b %d\", gridcolor=\"#EEEEEE\")\n",
    "fig.update_yaxes(tickformat=\".0%\", gridcolor=\"#EEEEEE\", row=1, col=1)\n",
    "fig.update_yaxes(gridcolor=\"#EEEEEE\", row=2, col=1, title=\"Cumulative units\")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/10_bias_analysis.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary Table for Slides\n",
    "\n",
    "Copy-pasteable table for PowerPoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "summary = scorecard[[\"rank\", \"model\", \"wape\", \"mae\", \"bias\"]].copy()\n",
    "summary[\"wape\"] = summary[\"wape\"].map(\"{:.1%}\".format)\n",
    "summary[\"mae\"] = summary[\"mae\"].map(\"{:.1f}\".format)\n",
    "summary[\"bias\"] = summary[\"bias\"].map(\"{:+.1f}\".format)\n",
    "\n",
    "# Traffic light emoji for slides\n",
    "def traffic_light(wape_str):\n",
    "    w = float(wape_str.strip(\"%\")) / 100\n",
    "    if w < 0.35:\n",
    "        return \"GREEN\"\n",
    "    elif w < 0.50:\n",
    "        return \"YELLOW\"\n",
    "    return \"RED\"\n",
    "\n",
    "summary[\"status\"] = summary[\"wape\"].apply(traffic_light)\n",
    "summary = summary.rename(columns={\n",
    "    \"rank\": \"#\",\n",
    "    \"model\": \"Model\",\n",
    "    \"wape\": \"WAPE\",\n",
    "    \"mae\": \"MAE\",\n",
    "    \"bias\": \"Bias\",\n",
    "    \"status\": \"Status\",\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY FOR SLIDES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test Period: {test['ds'].min().date()} to {test['ds'].max().date()} ({test['ds'].nunique()} weeks)\")\n",
    "print(f\"Series: {test['unique_id'].nunique()} Customer-Material combinations\")\n",
    "print(f\"Best Model: {best_model['model']}\")\n",
    "print(f\"Improvement: {improvement_pct:.0f}% better than current predictions\")\n",
    "print(f\"Estimated Impact: ${dollar_impact:,.0f}/year\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table as a clean Plotly figure (screenshot-ready)\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(summary.columns),\n",
    "        fill_color=BRAND[\"blue\"],\n",
    "        font=dict(color=\"white\", size=14),\n",
    "        align=\"center\",\n",
    "        height=36,\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[summary[c] for c in summary.columns],\n",
    "        fill_color=[\n",
    "            [BRAND[\"light\"]] * len(summary),  # #\n",
    "            [BRAND[\"light\"]] * len(summary),  # Model\n",
    "            [BRAND[\"light\"]] * len(summary),  # WAPE\n",
    "            [BRAND[\"light\"]] * len(summary),  # MAE\n",
    "            [BRAND[\"light\"]] * len(summary),  # Bias\n",
    "            [[BRAND[\"green\"] if s == \"GREEN\" else BRAND[\"yellow\"] if s == \"YELLOW\" else BRAND[\"red\"]\n",
    "              for s in summary[\"Status\"]]],  # Status\n",
    "        ],\n",
    "        font=dict(size=13),\n",
    "        align=\"center\",\n",
    "        height=30,\n",
    "    ),\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Model Scorecard</b>\",\n",
    "        font=dict(size=18, color=BRAND[\"dark\"]),\n",
    "    ),\n",
    "    height=max(200, len(summary) * 32 + 120),\n",
    "    width=CHART_WIDTH,\n",
    "    margin=dict(t=60, b=20, l=20, r=20),\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{EXPORT_DIR}/11_scorecard_table.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export All Charts\n",
    "\n",
    "All charts have been auto-saved as interactive HTML to `data/raw/charts/`.  \n",
    "\n",
    "For static PNG (for PowerPoint), uncomment and run below.  \n",
    "Requires `kaleido`: `pip install kaleido`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to export all charts as PNG for slides\n",
    "# import glob\n",
    "# for html_file in sorted(glob.glob(f\"{EXPORT_DIR}/*.html\")):\n",
    "#     fig = pio.read_json(html_file.replace(\".html\", \".json\"))  # won't work directly\n",
    "#     # Instead, re-run each fig.write_image() call above\n",
    "#\n",
    "# # Or export specific figures:\n",
    "# fig.write_image(f\"{EXPORT_DIR}/02_model_comparison.png\", width=900, height=500, scale=2)\n",
    "\n",
    "print(f\"\\nAll charts saved to: {EXPORT_DIR}/\")\n",
    "print(\"Files:\")\n",
    "for f in sorted(os.listdir(EXPORT_DIR)):\n",
    "    if f.endswith(\".html\"):\n",
    "        print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Presentation Order (Recommended)\n",
    "\n",
    "| Slide | Chart | File | Talking Point |\n",
    "|-------|-------|------|---------------|\n",
    "| 1 | Executive Summary | `01_executive_summary.html` | \"We improved accuracy by X%, worth $Y/year\" |\n",
    "| 2 | Model Comparison | `02_model_comparison.html` | \"We tested N approaches — [best model] wins\" |\n",
    "| 3 | Weekly Trendline | `03_weekly_accuracy_trendline.html` | \"The improvement is consistent week over week\" |\n",
    "| 4 | Cumulative Accuracy | `04_cumulative_accuracy.html` | \"As we accumulate more weeks, the gap holds\" |\n",
    "| 5 | Fiscal Month View | `05_fiscal_month_accuracy.html` | \"It works across fiscal periods\" |\n",
    "| 6 | Traffic Light | `06_traffic_light.html` | \"More products moved from red/yellow to green\" |\n",
    "| 7 | Top Products | `07_top_products_forecast_vs_actual.html` | \"Here are our biggest products — model tracks closely\" |\n",
    "| 8 | Improvements | `08_improvements_and_misses.html` | \"Biggest wins and where we still need work\" |\n",
    "| 9 | Hierarchy Accuracy | `09_accuracy_by_hierarchy.html` | \"Accuracy is strong at every level of rollup\" |\n",
    "| 10 | Bias Analysis | `10_bias_analysis.html` | \"Less systematic bias = fewer inventory surprises\" |\n",
    "| 11 | Scorecard | `11_scorecard_table.html` | \"Full scorecard for reference\" |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}